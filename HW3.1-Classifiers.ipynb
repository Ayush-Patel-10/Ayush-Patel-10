{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayush-Patel-10/Ayush-Patel-10/blob/main/HW3.1-Classifiers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d326d7d",
      "metadata": {
        "id": "3d326d7d"
      },
      "source": [
        "This can be run [run on Google Colab using this link](https://colab.research.google.com/github/CS7150/CS7150-Homework_3/blob/main/HW3.1-Classifiers.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dd40e9d",
      "metadata": {
        "id": "4dd40e9d"
      },
      "source": [
        "## MNIST Classifiers (Convolutional Neural Networks and Fully Connected Networks)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea20290e",
      "metadata": {
        "id": "ea20290e"
      },
      "source": [
        "<b>Optional</b>: Installing Wandb to see cool analysis of you code. You can go through the documentation here. We will do it for this assignment to get a taste of the GPU and CPU utilizations. If this is creating problems to your code, please comment out all the wandb lines from the notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0f63cd31",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f63cd31",
        "outputId": "2a0a0c78-83e8-4aac-f12c-2f83fe587791"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.15.12)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.40)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.32.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.10/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n"
          ]
        }
      ],
      "source": [
        "# Uncomment the below line to install wandb (optinal)\n",
        "!pip install wandb\n",
        "# Uncomment the below line to install torchinfo (https://github.com/TylerYep/torchinfo) [Mandatory]\n",
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8211a22f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8211a22f",
        "outputId": "6989eb11-4ea1-402d-fc40-e9f9765f9beb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "--2023-10-18 18:01:28--  https://cs7150.baulab.info/2022-Fall/data/mnist-classify.pth\n",
            "Resolving cs7150.baulab.info (cs7150.baulab.info)... 35.232.255.106\n",
            "Connecting to cs7150.baulab.info (cs7150.baulab.info)|35.232.255.106|:443... connected.\n",
            "HTTP request sent, awaiting response... 304 Not Modified\n",
            "File ‘mnist-classify.pth’ not modified on server. Omitting download.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "wget -N https://cs7150.baulab.info/2022-Fall/data/mnist-classify.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "621c2c89",
      "metadata": {
        "scrolled": true,
        "id": "621c2c89"
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader,random_split,Subset\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchinfo import summary\n",
        "import numpy as np\n",
        "import datetime\n",
        "\n",
        "from typing import List\n",
        "from collections import OrderedDict\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e05fd6b2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "e05fd6b2",
        "outputId": "fdf26947-c969-48d0-c824-f1a67872d5d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mayushpatel1323\u001b[0m (\u001b[33mdeeplearning_neu\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.12"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20231018_180134-bazbo274</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/deeplearning_neu/hw3.1-ConvNets/runs/bazbo274' target=\"_blank\">dainty-darkness-3</a></strong> to <a href='https://wandb.ai/deeplearning_neu/hw3.1-ConvNets' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/deeplearning_neu/hw3.1-ConvNets' target=\"_blank\">https://wandb.ai/deeplearning_neu/hw3.1-ConvNets</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/deeplearning_neu/hw3.1-ConvNets/runs/bazbo274' target=\"_blank\">https://wandb.ai/deeplearning_neu/hw3.1-ConvNets/runs/bazbo274</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/deeplearning_neu/hw3.1-ConvNets/runs/bazbo274?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f3e9b421f60>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Create an account at https://wandb.ai/site and paste the api key here (optional)\n",
        "import wandb\n",
        "wandb.init(project=\"hw3.1-ConvNets\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53ab5423",
      "metadata": {
        "id": "53ab5423"
      },
      "source": [
        "### Some helper functions to view network parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5233e6cb",
      "metadata": {
        "id": "5233e6cb"
      },
      "outputs": [],
      "source": [
        "def view_network_parameters(model):\n",
        "    # Visualise the number of parameters\n",
        "    tensor_list = list(model.state_dict().items())\n",
        "    total_parameters = 0\n",
        "    print('Model Summary\\n')\n",
        "    for layer_tensor_name, tensor in tensor_list:\n",
        "        total_parameters += int(torch.numel(tensor))\n",
        "        print('{}: {} elements'.format(layer_tensor_name, torch.numel(tensor)))\n",
        "    print(f'\\nTotal Trainable Parameters: {total_parameters}!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "13a073a7",
      "metadata": {
        "id": "13a073a7"
      },
      "outputs": [],
      "source": [
        "def view_network_shapes(model, input_shape):\n",
        "    print(summary(conv_net, input_size=input_shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d140bdae",
      "metadata": {
        "id": "d140bdae"
      },
      "source": [
        "### Fully Connected Network for Image Classification\n",
        "Let's build a simple fully connected network!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ccf368fb",
      "metadata": {
        "id": "ccf368fb"
      },
      "outputs": [],
      "source": [
        "def simple_fc_net():\n",
        "    model = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(1*28*28,8*28*28),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(8*28*28,16*14*14),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(16*14*14,32*7*7),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(32*7*7,288),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(288,64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64,10),\n",
        "        nn.LogSoftmax())\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8306548c",
      "metadata": {
        "id": "8306548c"
      },
      "outputs": [],
      "source": [
        "fc_net = simple_fc_net()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "f968a3c1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f968a3c1",
        "outputId": "18db5253-565e-4068-eecc-f0fecaf67d05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Summary\n",
            "\n",
            "1.weight: 4917248 elements\n",
            "1.bias: 6272 elements\n",
            "3.weight: 19668992 elements\n",
            "3.bias: 3136 elements\n",
            "5.weight: 4917248 elements\n",
            "5.bias: 1568 elements\n",
            "7.weight: 451584 elements\n",
            "7.bias: 288 elements\n",
            "9.weight: 18432 elements\n",
            "9.bias: 64 elements\n",
            "11.weight: 640 elements\n",
            "11.bias: 10 elements\n",
            "\n",
            "Total Trainable Parameters: 29985482!\n"
          ]
        }
      ],
      "source": [
        "view_network_parameters(fc_net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a19d63c2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a19d63c2",
        "outputId": "f2dc3942-a6ae-4327-8ac5-cde7081ab7db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "Sequential                               [1, 10]                   --\n",
              "├─Flatten: 1-1                           [1, 784]                  --\n",
              "├─Linear: 1-2                            [1, 6272]                 4,923,520\n",
              "├─ReLU: 1-3                              [1, 6272]                 --\n",
              "├─Linear: 1-4                            [1, 3136]                 19,672,128\n",
              "├─ReLU: 1-5                              [1, 3136]                 --\n",
              "├─Linear: 1-6                            [1, 1568]                 4,918,816\n",
              "├─ReLU: 1-7                              [1, 1568]                 --\n",
              "├─Linear: 1-8                            [1, 288]                  451,872\n",
              "├─ReLU: 1-9                              [1, 288]                  --\n",
              "├─Linear: 1-10                           [1, 64]                   18,496\n",
              "├─ReLU: 1-11                             [1, 64]                   --\n",
              "├─Linear: 1-12                           [1, 10]                   650\n",
              "├─LogSoftmax: 1-13                       [1, 10]                   --\n",
              "==========================================================================================\n",
              "Total params: 29,985,482\n",
              "Trainable params: 29,985,482\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 29.99\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.09\n",
              "Params size (MB): 119.94\n",
              "Estimated Total Size (MB): 120.04\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "from torchinfo import summary\n",
        "summary(fc_net, input_size=(1, 1, 28,28))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25cfc411",
      "metadata": {
        "id": "25cfc411"
      },
      "source": [
        "<b>Exercise</b>: Now try to add different layers and see how the network parameters vary. Does adding layers reduce the parameters? Does the number of hidden neurons in the layers affect the total trainable parameters?\n",
        "\n",
        "<i>Add a few sentences on your observations while using various architectures</i>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "2775c829",
      "metadata": {
        "id": "2775c829"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "def extended_fc_net():\n",
        "    model = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(1*14*14, 32*14*14),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(32*14*14, 16*14*14),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(16*14*14, 32*7*7),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm1d(32*7*7),\n",
        "        nn.Linear(32*7*7, 288),\n",
        "        nn.ReLU(),\n",
        "        nn.LayerNorm(288),\n",
        "        nn.Linear(288, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64, 10),\n",
        "        nn.ReLU(),\n",
        "        nn.LogSoftmax()\n",
        "    )\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fc_net_1 = extended_fc_net()"
      ],
      "metadata": {
        "id": "ZbZ7VVDa46oV"
      },
      "id": "ZbZ7VVDa46oV",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "view_network_parameters(fc_net_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwdGDuoj46rX",
        "outputId": "1a58d961-664b-4ad9-af3d-3bbf7b7f1f9b"
      },
      "id": "mwdGDuoj46rX",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Summary\n",
            "\n",
            "1.weight: 1229312 elements\n",
            "1.bias: 6272 elements\n",
            "3.weight: 19668992 elements\n",
            "3.bias: 3136 elements\n",
            "6.weight: 4917248 elements\n",
            "6.bias: 1568 elements\n",
            "8.weight: 1568 elements\n",
            "8.bias: 1568 elements\n",
            "8.running_mean: 1568 elements\n",
            "8.running_var: 1568 elements\n",
            "8.num_batches_tracked: 1 elements\n",
            "9.weight: 451584 elements\n",
            "9.bias: 288 elements\n",
            "11.weight: 288 elements\n",
            "11.bias: 288 elements\n",
            "12.weight: 18432 elements\n",
            "12.bias: 64 elements\n",
            "14.weight: 640 elements\n",
            "14.bias: 10 elements\n",
            "\n",
            "Total Trainable Parameters: 26304395!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "summary(fc_net_1, input_size=(1, 1, 14, 14))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6ORrRRY46u3",
        "outputId": "bfa1cd15-8391-460f-d391-36ad178b11a9"
      },
      "id": "m6ORrRRY46u3",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "Sequential                               [1, 10]                   --\n",
              "├─Flatten: 1-1                           [1, 196]                  --\n",
              "├─Linear: 1-2                            [1, 6272]                 1,235,584\n",
              "├─ReLU: 1-3                              [1, 6272]                 --\n",
              "├─Linear: 1-4                            [1, 3136]                 19,672,128\n",
              "├─ReLU: 1-5                              [1, 3136]                 --\n",
              "├─Dropout: 1-6                           [1, 3136]                 --\n",
              "├─Linear: 1-7                            [1, 1568]                 4,918,816\n",
              "├─ReLU: 1-8                              [1, 1568]                 --\n",
              "├─BatchNorm1d: 1-9                       [1, 1568]                 3,136\n",
              "├─Linear: 1-10                           [1, 288]                  451,872\n",
              "├─ReLU: 1-11                             [1, 288]                  --\n",
              "├─LayerNorm: 1-12                        [1, 288]                  576\n",
              "├─Linear: 1-13                           [1, 64]                   18,496\n",
              "├─ReLU: 1-14                             [1, 64]                   --\n",
              "├─Linear: 1-15                           [1, 10]                   650\n",
              "├─ReLU: 1-16                             [1, 10]                   --\n",
              "├─LogSoftmax: 1-17                       [1, 10]                   --\n",
              "==========================================================================================\n",
              "Total params: 26,301,258\n",
              "Trainable params: 26,301,258\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 26.30\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.11\n",
              "Params size (MB): 105.21\n",
              "Estimated Total Size (MB): 105.31\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Observation:**\n",
        "\n",
        "In the extended neural network architecture, adding layers like dropout, batch normalization, and layer normalization, along with a reduction in the number of hidden neurons in some layers, resulted in a decreased total number of trainable parameters compared to the original model. These architectural changes contribute to reducing model complexity and the risk of overfitting by introducing regularization and scaling techniques. The specific impact of these changes on parameter count includes fewer parameters in dropout layers and additional parameters for scaling and shifting in normalization layers. It's essential to choose architectural modifications carefully based on the problem's complexity and data availability to strike the right balance between model complexity and generalization."
      ],
      "metadata": {
        "id": "o4lXELQb9rAP"
      },
      "id": "o4lXELQb9rAP"
    },
    {
      "cell_type": "markdown",
      "id": "245bc885",
      "metadata": {
        "id": "245bc885"
      },
      "source": [
        "### Convolutional Neural Network for Image Classification\n",
        "Let's build a simple CNN to classify our images.\n",
        "<b> Exercise 3.1.1:</b> In the function below please add the conv/Relu/Maxpool layers to match the shape of FC-Net. Suppose at the some layer the FC-Net has `28*28*16` dimension, we want your conv_net to have `16 X 28 X 28` shape at the same numbered layer. <br>\n",
        "<b>Extra-credit:</b> Try not to use MaxPool2d !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "65674742",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "65674742",
        "outputId": "3ee73592-3182-4abd-f296-507aeee063a8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-21-3e7eaa19b65b>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    '''\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
          ]
        }
      ],
      "source": [
        "def simple_conv_net():\n",
        "    model = nn.Sequential(\n",
        "        nn.Conv2d(1,16,kernel_size=3,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2,2),\n",
        "        # TO-DO: Add layers below\n",
        "        '''\n",
        "        Add your code here to match the output shape of the FC-Net\n",
        "        '''\n",
        "        # TO-DO, what will your shape be after you flatten? Fill it in place of None\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(None,64),\n",
        "        # Do not change the code below\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64,10),\n",
        "        nn.LogSoftmax())\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "def simple_conv_net():\n",
        "    model = nn.Sequential(\n",
        "        nn.Conv2d(1, 16, kernel_size = 3, padding = 1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(16, 16, kernel_size = 3, padding = 1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(16, 16, kernel_size = 3, padding = 1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(16, 16, kernel_size = 3, padding = 1),\n",
        "        nn.ReLU(),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(16 * 28 * 28, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64, 10),\n",
        "        nn.LogSoftmax()\n",
        "    )\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "W02wROT_-6bI"
      },
      "id": "W02wROT_-6bI",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "6bb06a8d",
      "metadata": {
        "id": "6bb06a8d"
      },
      "outputs": [],
      "source": [
        "conv_net = simple_conv_net()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "c6d6e8f0",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6d6e8f0",
        "outputId": "dd5747ae-6613-4092-a00b-75d0d556a986"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Summary\n",
            "\n",
            "0.weight: 144 elements\n",
            "0.bias: 16 elements\n",
            "2.weight: 2304 elements\n",
            "2.bias: 16 elements\n",
            "4.weight: 2304 elements\n",
            "4.bias: 16 elements\n",
            "6.weight: 2304 elements\n",
            "6.bias: 16 elements\n",
            "9.weight: 802816 elements\n",
            "9.bias: 64 elements\n",
            "11.weight: 640 elements\n",
            "11.bias: 10 elements\n",
            "\n",
            "Total Trainable Parameters: 810650!\n"
          ]
        }
      ],
      "source": [
        "view_network_parameters(conv_net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "49ed1786",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49ed1786",
        "outputId": "62577636-86a0-44b0-ba02-527c2b1bdb43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================================================================\n",
            "Layer (type:depth-idx)                   Output Shape              Param #\n",
            "==========================================================================================\n",
            "Sequential                               [1, 10]                   --\n",
            "├─Conv2d: 1-1                            [1, 16, 28, 28]           160\n",
            "├─ReLU: 1-2                              [1, 16, 28, 28]           --\n",
            "├─Conv2d: 1-3                            [1, 16, 28, 28]           2,320\n",
            "├─ReLU: 1-4                              [1, 16, 28, 28]           --\n",
            "├─Conv2d: 1-5                            [1, 16, 28, 28]           2,320\n",
            "├─ReLU: 1-6                              [1, 16, 28, 28]           --\n",
            "├─Conv2d: 1-7                            [1, 16, 28, 28]           2,320\n",
            "├─ReLU: 1-8                              [1, 16, 28, 28]           --\n",
            "├─Flatten: 1-9                           [1, 12544]                --\n",
            "├─Linear: 1-10                           [1, 64]                   802,880\n",
            "├─ReLU: 1-11                             [1, 64]                   --\n",
            "├─Linear: 1-12                           [1, 10]                   650\n",
            "├─LogSoftmax: 1-13                       [1, 10]                   --\n",
            "==========================================================================================\n",
            "Total params: 810,650\n",
            "Trainable params: 810,650\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (M): 6.39\n",
            "==========================================================================================\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.40\n",
            "Params size (MB): 3.24\n",
            "Estimated Total Size (MB): 3.65\n",
            "==========================================================================================\n"
          ]
        }
      ],
      "source": [
        "view_network_shapes(conv_net, input_shape=(1,1,28,28))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f577403",
      "metadata": {
        "id": "0f577403"
      },
      "source": [
        "<b>Exercise 3.1.2</b>: Why is the final layer a log softmax? What is a softmax function? Can we use ReLU instead of softmax? If yes, what would you do different? If not, tell us why. If you think there is a different answer, feel free to use this space to chart it down"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e530f3e2",
      "metadata": {
        "id": "e530f3e2"
      },
      "source": [
        " Please type your answer here ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41ea2c8c",
      "metadata": {
        "id": "41ea2c8c"
      },
      "source": [
        "<b>Exercise 3.1.3</b>: What is the ratio of number of parameters of Conv-net to number of parameters of FC-Net <br>\n",
        "$\\frac{p_{conv-net}}{p_{fc-net}}$ = Fill your answer <br>\n",
        "Do you see the difference ?!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4c6329e",
      "metadata": {
        "id": "b4c6329e"
      },
      "source": [
        "<b>Exercise 3.1.4</b>: Now try to add different layers and see how the network parameters vary. Does adding layers reduce the parameters? Does the number of hidden neurons in the layers affect the total trainable parameters? Use the `build_custom_fc_net` function given below. You do not have to understand the working of it.\n",
        "\n",
        "<i>Add a few sentences on your observations while using various architectures</i>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b89d76f7",
      "metadata": {
        "scrolled": true,
        "id": "b89d76f7"
      },
      "outputs": [],
      "source": [
        "def build_custom_fc_net(inp_dim: int, out_dim: int, hidden_fc_dim: List[int]):\n",
        "    '''\n",
        "    Inputs :\n",
        "\n",
        "    inp_dim: Shape of the input dimensions (in MNIST case 28*28)\n",
        "    out_dim: Desired classification classes (in MNIST case 10)\n",
        "    hidden_fc_dim: List of the intermediate dimension shapes (list of integers). Try different values and see the shapes'\n",
        "\n",
        "    Return: nn.Sequential (final custom model)\n",
        "    '''\n",
        "    assert type(hidden_fc_dim) == list, \"Please define hidden_fc_dim as list of integers\"\n",
        "    layers = []\n",
        "    layers.append((f'flatten', nn.Flatten()))\n",
        "    # If no hidden layer is required\n",
        "    if len(hidden_fc_dim) == 0:\n",
        "        layers.append((f'linear',nn.Linear(math.prod(inp_dim),out_dim)))\n",
        "        layers.append((f'activation',nn.LogSoftmax()))\n",
        "    else:\n",
        "        # Loop over hidden dimensions and add layers\n",
        "        for idx, dim in enumerate(hidden_fc_dim):\n",
        "            if idx == 0:\n",
        "                layers.append((f'linear_{idx+1}',nn.Linear(math.prod(inp_dim),dim)))\n",
        "                layers.append((f'activation_{idx+1}',nn.ReLU()))\n",
        "            else:\n",
        "                layers.append((f'linear_{idx+1}',nn.Linear(hidden_fc_dim[idx-1],dim)))\n",
        "                layers.append((f'activation_{idx+1}',nn.ReLU()))\n",
        "        layers.append((f'linear_{idx+2}',nn.Linear(dim,out_dim)))\n",
        "        layers.append((f'activation_{idx+2}',nn.LogSoftmax()))\n",
        "\n",
        "    model =  nn.Sequential(OrderedDict(layers))\n",
        "    return model\n",
        "\n",
        "# TO-DO build different networks (atleast 3) and see the parameters\n",
        "#(You don't have to understand the function above. It is a generic way to build a FC-Net)\n",
        "\n",
        "\n",
        "fc_net_custom1 = build_custom_fc_net(inp_dim=(1,28,28), out_dim=10, hidden_fc_dim=[128,64,32])\n",
        "view_network_parameters(fc_net_custom1)\n",
        "\n",
        "# fc_net_custom2 =\n",
        "# view_network_parameters(fc_net_custom2)\n",
        "\n",
        "# fc_net_custom3 =\n",
        "# view_network_parameters(fc_net_custom3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71c48647",
      "metadata": {
        "id": "71c48647"
      },
      "source": [
        "## Let's train the models to see their performace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94ae412d",
      "metadata": {
        "id": "94ae412d"
      },
      "outputs": [],
      "source": [
        "# downloading mnist into folder\n",
        "data_dir = 'data' # make sure that this folder is created in your working dir\n",
        "# transform the PIL images to tensor using torchvision.transform.toTensor method\n",
        "train_data = torchvision.datasets.MNIST(data_dir, train=True, download=True, transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor()]))\n",
        "test_data  = torchvision.datasets.MNIST(data_dir, train=False, download=True, transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor()]))\n",
        "print(f'Datatype of the dataset object: {type(train_data)}')\n",
        "# check the length of dataset\n",
        "n_train_samples = len(train_data)\n",
        "print(f'Number of samples in training data: {len(train_data)}')\n",
        "print(f'Number of samples in test data: {len(test_data)}')\n",
        "# Check the format of dataset\n",
        "#print(f'Foramt of the dataset: \\n {train_data}')\n",
        "\n",
        "val_split = .2\n",
        "batch_size=256\n",
        "\n",
        "train_data_, val_data = random_split(train_data, [int(n_train_samples*(1-val_split)), int(n_train_samples*val_split)])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data_, batch_size=batch_size,shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size,shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b94c083",
      "metadata": {
        "id": "0b94c083"
      },
      "source": [
        "### Displaying the loaded dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6e15476",
      "metadata": {
        "id": "a6e15476"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure()\n",
        "for i in range(6):\n",
        "  plt.subplot(2, 3, i+1)\n",
        "  plt.tight_layout()\n",
        "  plt.imshow(train_data[i][0][0], cmap='gray', interpolation='none')\n",
        "  plt.title(\"Class Label: {}\".format(train_data[i][1]))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "524851bc",
      "metadata": {
        "id": "524851bc"
      },
      "source": [
        "## Function to train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "718c5a83",
      "metadata": {
        "id": "718c5a83"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, device, loss_fn, optimizer, input_dim=(-1,1,28,28)):\n",
        "    model.train()\n",
        "    # Initiate a loss monitor\n",
        "    train_loss = []\n",
        "    # Iterate the dataloader (we do not need the label values, this is unsupervised learning and not supervised classification)\n",
        "    for images, labels in train_loader: # the variable `labels` will be used for customised training\n",
        "        # reshape input\n",
        "        images = torch.reshape(images,input_dim)\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # predict the class\n",
        "        predicted = model(images)\n",
        "        loss = loss_fn(predicted, labels)\n",
        "        # Backward pass (back propagation)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        wandb.log({\"Training Loss\": loss})\n",
        "        wandb.watch(model)\n",
        "        train_loss.append(loss.detach().cpu().numpy())\n",
        "    return np.mean(train_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "802d74a4",
      "metadata": {
        "id": "802d74a4"
      },
      "source": [
        "## Function to test the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f24a06a",
      "metadata": {
        "id": "9f24a06a"
      },
      "outputs": [],
      "source": [
        "# Testing Function\n",
        "def test_model(model, test_loader, device, loss_fn, input_dim=(-1,1,28,28)):\n",
        "    # Set evaluation mode for encoder and decoder\n",
        "    model.eval()\n",
        "    with torch.no_grad(): # No need to track the gradients\n",
        "        # Define the lists to store the outputs for each batch\n",
        "        predicted = []\n",
        "        actual = []\n",
        "        for images, labels in test_loader:\n",
        "            # reshape input\n",
        "            images = torch.reshape(images,input_dim)\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            ## predict the label\n",
        "            pred = model(images)\n",
        "            # Append the network output and the original image to the lists\n",
        "            predicted.append(pred.cpu())\n",
        "            actual.append(labels.cpu())\n",
        "        # Create a single tensor with all the values in the lists\n",
        "        predicted = torch.cat(predicted)\n",
        "        actual = torch.cat(actual)\n",
        "        # Evaluate global loss\n",
        "        val_loss = loss_fn(predicted, actual)\n",
        "    return val_loss.data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfdd1864",
      "metadata": {
        "id": "bfdd1864"
      },
      "source": [
        "Before we start training let's delete the huge FC-Net we built and build a reasonable FC-Net (You learnt why such larger networks are not reasonable in the previous notebook)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51fca7cc",
      "metadata": {
        "id": "51fca7cc"
      },
      "outputs": [],
      "source": [
        "del fc_net, fc_net_custom1, fc_net_custom2, fc_net_custom3\n",
        "torch.cuda.empty_cache()\n",
        "# Building a reasonable fully connected network\n",
        "fc_net = build_custom_fc_net(inp_dim=(1,28,28), out_dim=10, hidden_fc_dim=[128,64,32])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bc501cf",
      "metadata": {
        "id": "6bc501cf"
      },
      "source": [
        "<b>Exercise 3.1.5:</b>\n",
        "Code the `weight_init_xavier` function by referring to https://pytorch.org/docs/stable/nn.init.html. Replace the weight initializations to your own function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60636ac1",
      "metadata": {
        "id": "60636ac1"
      },
      "outputs": [],
      "source": [
        "### Set the random seed for reproducible results\n",
        "torch.manual_seed(0)\n",
        "# Choosing a device based on the env and torch setup\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(f'Selected device: {device}')\n",
        "\n",
        "def weight_init_zero(m):\n",
        "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "        torch.nn.init.constant_(m.weight, 0.0)\n",
        "        m.bias.data.fill_(0.01)\n",
        "\n",
        "def weight_init_xavier(m):\n",
        "    '''\n",
        "    TO-DO: please add code below to add xavier uniform initialization and remove the 'pass'\n",
        "    '''\n",
        "    pass\n",
        "\n",
        "\n",
        "fc_net.to(device)\n",
        "conv_net.to(device)\n",
        "\n",
        "# Apply the weight initialization\n",
        "fc_net.apply(weight_init_zero)\n",
        "conv_net.apply(weight_init_zero)\n",
        "\n",
        "# Apply the xavier weight initialization\n",
        "#TO-DO: Add your function here\n",
        "fc_net.apply(weight_init_xavier)\n",
        "conv_net.apply(weight_init_xavier)\n",
        "\n",
        "\n",
        "# Take the parameters for optimiser\n",
        "params_to_optimize_fc = [\n",
        "    {'params': fc_net.parameters()}\n",
        "]\n",
        "\n",
        "params_to_optimize_conv = [\n",
        "    {'params': conv_net.parameters()}\n",
        "]\n",
        "### Define the loss function\n",
        "loss_fn = torch.nn.NLLLoss()\n",
        "### Define an optimizer (both for the encoder and the decoder!)\n",
        "lr= 0.001\n",
        "\n",
        "optim_fc = torch.optim.Adam(params_to_optimize_fc, lr=lr, weight_decay=1e-05)\n",
        "optim_conv = torch.optim.Adam(params_to_optimize_conv, lr=lr, weight_decay=1e-05)\n",
        "num_epochs = 30\n",
        "wandb.config = {\n",
        "  \"learning_rate\": lr,\n",
        "  \"epochs\": num_epochs,\n",
        "  \"batch_size\": batch_size\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53753ff1",
      "metadata": {
        "id": "53753ff1"
      },
      "source": [
        "# Training the Convolutional Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9616b62",
      "metadata": {
        "scrolled": true,
        "id": "a9616b62"
      },
      "outputs": [],
      "source": [
        "print('Conv Net training started')\n",
        "history_conv = {'train_loss':[],'val_loss':[]}\n",
        "start_time = datetime.datetime.now()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    ### Training\n",
        "\n",
        "    train_loss = train_model(\n",
        "        model=conv_net,\n",
        "        train_loader=train_loader,\n",
        "        device=device,\n",
        "        loss_fn=loss_fn,\n",
        "        optimizer=optim_conv,\n",
        "        input_dim=(-1,1,28,28))\n",
        "    ### Validation  (use the testing function)\n",
        "    val_loss = test_model(\n",
        "        model=conv_net,\n",
        "        test_loader=test_loader,\n",
        "        device=device,\n",
        "        loss_fn=loss_fn,\n",
        "        input_dim=(-1,1,28,28))\n",
        "    # Print Losses\n",
        "    print(f'Epoch {epoch+1}/{num_epochs} : train loss {train_loss:.3f} \\t val loss {val_loss:.3f}')\n",
        "    history_conv['train_loss'].append(train_loss)\n",
        "    history_conv['val_loss'].append(val_loss)\n",
        "\n",
        "\n",
        "print(f'Conv Net training done in {(datetime.datetime.now()-start_time).total_seconds():.3f} seconds!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1465fbd",
      "metadata": {
        "id": "f1465fbd"
      },
      "source": [
        "### Visualizing Training Progress of Conv Net (Also check out your wandb.ai homepage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1622763",
      "metadata": {
        "id": "c1622763"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "plt.plot(history_conv['train_loss'], color='blue')\n",
        "plt.plot(history_conv['val_loss'], color='red')\n",
        "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Negative Log Likelihood Loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f163e64a",
      "metadata": {
        "id": "f163e64a"
      },
      "source": [
        "### Visualizing Predictions of Conv Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89ac3520",
      "metadata": {
        "scrolled": false,
        "id": "89ac3520"
      },
      "outputs": [],
      "source": [
        "examples = enumerate(test_loader)\n",
        "batch_idx, (example_data, example_targets) = next(examples)\n",
        "with torch.no_grad():\n",
        "    example_data = example_data.to(device)\n",
        "    output = conv_net(example_data)\n",
        "example_data = example_data.cpu().detach().numpy()\n",
        "fig = plt.figure(figsize=(5,5))\n",
        "for i in range(9):\n",
        "    plt.subplot(3,3,i+1)\n",
        "    plt.tight_layout()\n",
        "    plt.imshow(example_data[i][0], cmap='gray',interpolation='none')\n",
        "    plt.title(\"Prediction: {}\".format(\n",
        "    output.data.max(1, keepdim=True)[1][i].item()))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eacd45fe",
      "metadata": {
        "id": "eacd45fe"
      },
      "source": [
        "# Training the Fully-Connected Neural Networks\n",
        "\n",
        "<b>Exercise 3.1.6:</b> Train the fully connected neural network and analyse it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ee8d12c",
      "metadata": {
        "id": "4ee8d12c"
      },
      "outputs": [],
      "source": [
        "#TO-DO:Train the fc_net here\n",
        "print('FC Net training started')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4c5cd18",
      "metadata": {
        "id": "b4c5cd18"
      },
      "source": [
        "## Visualizing Training Progress of FC Net (Check out your wandb.ai project webpage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "926d5e38",
      "metadata": {
        "id": "926d5e38"
      },
      "outputs": [],
      "source": [
        "# TODO - Visualize the training progress of fc_net\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "107c77be",
      "metadata": {
        "id": "107c77be"
      },
      "source": [
        "## Visualizing Predictions of FC Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6f399e8",
      "metadata": {
        "id": "a6f399e8"
      },
      "outputs": [],
      "source": [
        "# TODO - Visualise the predictions of fc_net\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2981154",
      "metadata": {
        "id": "b2981154"
      },
      "source": [
        "<b>Exercise 3.1.7</b>: What are the training times for each of the model? Did both the models take similar times? If yes, why? Shouldn't CNN train faster given it's number of weights to train?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3528f2d9",
      "metadata": {
        "id": "3528f2d9"
      },
      "outputs": [],
      "source": [
        "#Please type your answer here ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b33605f1",
      "metadata": {
        "id": "b33605f1"
      },
      "source": [
        "## Let's see how the models perform under translation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a4d24b6",
      "metadata": {
        "id": "7a4d24b6"
      },
      "source": [
        "In principle, one of the advantages of convolutions is that they are equivariant under translation which means that a function composed out of convolutions should invariant under translation.\n",
        "\n",
        "<b>Exercise 3.1.8</b>: In practice, however, we might not see perfect invariance under translation.  What aspect of our network leads to imperfect invariance?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "889cbdfc",
      "metadata": {
        "id": "889cbdfc"
      },
      "source": [
        "Type your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8a0d35b",
      "metadata": {
        "id": "b8a0d35b"
      },
      "source": [
        "We will next measure the sensitivity  of the convolutional network to translation in practice, and we will compare it to the fully-connected version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e8583d7",
      "metadata": {
        "id": "9e8583d7"
      },
      "outputs": [],
      "source": [
        "## function to check accuracies for unit translation\n",
        "def shiftVsAccuracy(model, test_loader, device, loss_fn, shifts = 12, input_dim=(-1,1,28,28)):\n",
        "    # Set evaluation mode for encoder and decoder\n",
        "    accuracies = []\n",
        "    shifted = []\n",
        "    for i in range(-shifts,shifts):\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad(): # No need to track the gradients\n",
        "            # Define the lists to store the outputs for each batch\n",
        "            predicted = []\n",
        "            actual = []\n",
        "            for images, labels in test_loader:\n",
        "                # reshape input\n",
        "                images = torch.roll(images,shifts=i, dims=2)\n",
        "                if i == 0:\n",
        "                    pass\n",
        "                elif i > 0:\n",
        "                    images[:,:,:i,:] = 0\n",
        "                else:\n",
        "                    images[:,:,i:,:] = 0\n",
        "                images = torch.reshape(images,input_dim)\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "                ## predict the label\n",
        "                pred = model(images)\n",
        "                # Append the network output and the original image to the lists\n",
        "                _ , pred = torch.max(pred.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (pred == labels).sum().item()\n",
        "                predicted.append(pred.cpu())\n",
        "                actual.append(labels.cpu())\n",
        "            shifted.append(images[0][0].cpu())\n",
        "            acc = 100 * correct // total\n",
        "            accuracies.append(acc)\n",
        "    return accuracies,shifted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89576a2d",
      "metadata": {
        "id": "89576a2d"
      },
      "outputs": [],
      "source": [
        "accuracies,shifted = shiftVsAccuracy(\n",
        "        model=conv_net,\n",
        "        test_loader=test_loader,\n",
        "        device=device,\n",
        "        shifts=12,\n",
        "        loss_fn=loss_fn,\n",
        "        input_dim=(-1,1,28,28))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99e2b0c9",
      "metadata": {
        "id": "99e2b0c9"
      },
      "outputs": [],
      "source": [
        "shifts = np.arange(-12,12)\n",
        "plt.plot(shifts,accuracies)\n",
        "plt.title('Accuracy Vs Translation')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2320b45",
      "metadata": {
        "id": "b2320b45"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(20,20))\n",
        "plt_num = 0\n",
        "for i in range(-12,12):\n",
        "    plt.subplot(5,6,plt_num+1)\n",
        "    plt.imshow(shifted[plt_num], cmap='gray',interpolation='none')\n",
        "    plt.title(f\"Shifted: {i} Accuracy: {accuracies[plt_num]}\")\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt_num+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b217e71",
      "metadata": {
        "id": "6b217e71"
      },
      "source": [
        "<b>Exercise 3.1.8:</b>\n",
        "Do the same for FC-Net and plot the accuracies. Is the rate of accuracy degradation same as Conv-Net? Can you justify why this happened? <br>\n",
        "Clue: You might want to look at the way convolution layers process information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80b4cf9a",
      "metadata": {
        "id": "80b4cf9a"
      },
      "outputs": [],
      "source": [
        "# To-DO Write your code below"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}